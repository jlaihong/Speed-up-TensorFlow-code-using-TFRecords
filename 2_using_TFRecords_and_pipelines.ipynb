{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTMU565qzQS2"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import glob\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "import time\n",
    "\n",
    "import os\n",
    "\n",
    "# Extra imports\n",
    "import io\n",
    "from tensorflow.python.data.experimental import AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure same GPU assigned by colab, for fair comparison\n",
    "\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mcv9J5Oc29wo"
   },
   "source": [
    "# 1. Gather the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUMnEN6kNr5G"
   },
   "source": [
    "## 1.1 Download and Extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21546,
     "status": "ok",
     "timestamp": 1629441573942,
     "user": {
      "displayName": "Jeremy Lai Hong",
      "photoUrl": "",
      "userId": "06292101247589650245"
     },
     "user_tz": -120
    },
    "id": "c98Zdwld3157",
    "outputId": "52035c97-7766-4823-f7b6-33e49673b4d6"
   },
   "outputs": [],
   "source": [
    "!gdown https://drive.google.com/u/0/uc?id=1STYsoP85lyKAtarMRuDyTjp89tAbIDM-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o caltech256_subset_resized_cropped256x256.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths = sorted(glob.glob(\"caltech256_subset_resized_cropped256x256/data/*\"))[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZAk86KNNzwu"
   },
   "source": [
    "## 1.2 Split into training, validation and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_paths = [] \n",
    "shuffled_labels = []\n",
    "\n",
    "with open(\"caltech256_subset_resized_cropped256x256/shuffled_labels.txt\") as label_file:\n",
    "    label_file_lines = label_file.readlines()\n",
    "    \n",
    "for line in label_file_lines:\n",
    "    image_path, image_label = line.strip().split(\",\")\n",
    "    shuffled_paths.append(image_path)\n",
    "    shuffled_labels.append(int(image_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"caltech_dataset.tfrecords\") as writer:\n",
    "    for path, label in zip(shuffled_paths, shuffled_labels):\n",
    "        image = Image.open(\"caltech256_subset_resized_cropped256x256/data/\" + path)\n",
    "\n",
    "        bytes_buffer = io.BytesIO()\n",
    "        image.convert(\"RGB\").save(bytes_buffer, \"JPEG\")\n",
    "        image_bytes = bytes_buffer.getvalue()\n",
    "\n",
    "        bytes_feature = tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_bytes]))\n",
    "        class_feature = tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n",
    "\n",
    "        example = tf.train.Example(\n",
    "          features=tf.train.Features(feature={\n",
    "              \"image\": bytes_feature,\n",
    "              \"class\": class_feature\n",
    "          })\n",
    "        )\n",
    "\n",
    "        writer.write(example.SerializeToString())\n",
    "\n",
    "        image.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IQ-5DCEgRcRn"
   },
   "outputs": [],
   "source": [
    "image_feature_description = {\n",
    "    \"image\": tf.io.FixedLenFeature([], tf.string), \n",
    "    \"class\": tf.io.FixedLenFeature([], tf.int64), \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPkWzyTkRcVU"
   },
   "outputs": [],
   "source": [
    "def _parse_data(unparsed_example):\n",
    "    return tf.io.parse_single_example(unparsed_example, image_feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sFH7pvJGRecL"
   },
   "outputs": [],
   "source": [
    "def _bytestring_to_pixels(parsed_example):\n",
    "    byte_string = parsed_example['image']\n",
    "    image = tf.io.decode_image(byte_string)\n",
    "    image = tf.reshape(image, [256, 256, 3])\n",
    "    return image, parsed_example[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1RntJ7zReee"
   },
   "outputs": [],
   "source": [
    "def load_and_extract_images(filepath):\n",
    "    dataset = tf.data.TFRecordDataset(filepath)\n",
    "    dataset = dataset.map(_parse_data, num_parallel_calls=AUTOTUNE)\n",
    "    dataset = dataset.map(_bytestring_to_pixels, num_parallel_calls=AUTOTUNE) # .cache()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caltech_dataset = load_and_extract_images(\"caltech_dataset.tfrecords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.6\n",
    "validation_split = 0.2\n",
    "\n",
    "num_train_images = int(len(shuffled_paths) * train_split)\n",
    "num_validation_images = int(len(shuffled_paths) * validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VeK2RCg7RehG"
   },
   "outputs": [],
   "source": [
    "train_dataset = caltech_dataset.take(num_train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdkObOvhRmWf"
   },
   "outputs": [],
   "source": [
    "validation_dataset = caltech_dataset.skip(num_train_images).take(num_validation_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = caltech_dataset.skip(num_train_images + num_validation_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Prepare data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GzkBZogb5eZX"
   },
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "crop_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHg8aRNSR0HN"
   },
   "outputs": [],
   "source": [
    "def _train_data_preprocess_and_augment(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.image.random_crop(image, size=[crop_size, crop_size, 3])\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Kloo1IaR0Jg"
   },
   "outputs": [],
   "source": [
    "train_preprocessed_augmented = train_dataset.map(_train_data_preprocess_and_augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDxGwJ8nR420"
   },
   "outputs": [],
   "source": [
    "crop_offset = (image_size - crop_size) // 2\n",
    "\n",
    "def _test_data_preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32) #  - means\n",
    "    \n",
    "    center_crop = tf.image.crop_to_bounding_box(\n",
    "        image, crop_offset, crop_offset, crop_size, crop_size\n",
    "    )\n",
    "    \n",
    "    return center_crop, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6CsH0G8R45M"
   },
   "outputs": [],
   "source": [
    "validation_preprocessed = validation_dataset.map(_test_data_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrSnun4T3EtG"
   },
   "source": [
    "## 2. Build and Train a model for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbWEvMaHzdAf"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    inputs = layers.Input(shape=(crop_size, crop_size, 3))\n",
    "\n",
    "    pretrained_resnet_model = tf.keras.applications.resnet50.ResNet50(include_top=False, input_tensor=inputs)\n",
    "\n",
    "    pretrained_resnet_model.trainable = False\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(pretrained_resnet_model.output)\n",
    "\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    outputs = layers.Dense(256, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs, outputs, name=\"ResNet\")\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "    model.compile(\n",
    "      optimizer=opt, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7470,
     "status": "ok",
     "timestamp": 1629443625415,
     "user": {
      "displayName": "Jeremy Lai Hong",
      "photoUrl": "",
      "userId": "06292101247589650245"
     },
     "user_tz": -120
    },
    "id": "GZq7IB0i7LsH",
    "outputId": "1a1eabb6-8a85-4154-e407-46e01231102b"
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "initial_start = time.time()\n",
    "\n",
    "for epoch in range(10):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    for images, label_batch in train_preprocessed_augmented.batch(batch_size):\n",
    "        model.fit(images, label_batch, epochs=1, verbose=0, batch_size=batch_size)\n",
    "\n",
    "    print(f\"Evaluating Validation Accuracy...\")\n",
    "    \n",
    "    model.evaluate(validation_preprocessed.batch(batch_size))\n",
    "    \n",
    "    epoch_end = time.time()\n",
    "    \n",
    "    print(f\"Epoch Time: {epoch_end - epoch_start}\")\n",
    "\n",
    "last_end = time.time()\n",
    "\n",
    "print(last_end-initial_start)\n",
    "\n",
    "print(f\"Total Time: {last_end-initial_start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wcS9VWRO5b-"
   },
   "source": [
    "# 2. Alternative model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "initial_start = time.time()\n",
    "\n",
    "model.fit(train_preprocessed_augmented.batch(batch_size), validation_data=validation_preprocessed.batch(batch_size), epochs=10)\n",
    "\n",
    "last_end = time.time()\n",
    "\n",
    "print(last_end-initial_start)\n",
    "\n",
    "print(f\"Total Time: {last_end-initial_start}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNQpjVSvJvWV+bhPbkpHX1c",
   "collapsed_sections": [],
   "name": "2_using_TFRecords.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
